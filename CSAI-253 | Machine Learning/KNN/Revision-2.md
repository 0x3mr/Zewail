### Question 1:
*What is the primary goal of model evaluation in the machine learning workflow?*

   A. To collect more data  
   B. To test the model against unseen data and measure its performance  
   C. To clean and normalize the data  
   D. To select the best algorithm for the problem  

<details>
<summary>Answer</summary>
B. To test the model against unseen data and measure its performance
</details>

---

### Question 2:
*Which of the following is a common metric used for evaluating classification models?*

   A. Mean Squared Error (MSE)  
   B. Accuracy  
   C. R-squared  
   D. Mean Absolute Error (MAE)  

<details>
<summary>Answer</summary>
B. Accuracy
</details>

---

### Question 3:
*What is underfitting in machine learning?*

   A. When the model is too complex and learns the noise in the training data  
   B. When the model is too simple and cannot capture the underlying patterns  
   C. When the model performs well on unseen data but poorly on training data  
   D. When the model has too many features  

<details>
<summary>Answer</summary>
B. When the model is too simple and cannot capture the underlying patterns
</details>

---

### Question 4:
*What is overfitting in machine learning?*

   A. When the model is too simple and cannot capture the underlying patterns  
   B. When the model is too complex and learns the noise in the training data  
   C. When the model performs well on training data but poorly on unseen data  
   D. Both B and C  

<details>
<summary>Answer</summary>
D. Both B and C
</details>

---

### Question 5:
*Which of the following is a way to reduce overfitting?*

   A. Increase the complexity of the model  
   B. Use more training data  
   C. Reduce the number of features  
   D. Both B and C  

<details>
<summary>Answer</summary>
D. Both B and C
</details>

---

### Question 6:
*What is the relationship between bias and variance in machine learning?*

   A. High bias leads to underfitting, while high variance leads to overfitting  
   B. High bias leads to overfitting, while high variance leads to underfitting  
   C. Low bias leads to underfitting, while low variance leads to overfitting  
   D. Low bias leads to overfitting, while low variance leads to underfitting  

<details>
<summary>Answer</summary>
A. High bias leads to underfitting, while high variance leads to overfitting
</details>

---

### Question 7:
*What is the K-Nearest Neighbors (K-NN) algorithm used for?*

   A. Clustering  
   B. Classification and regression  
   C. Reinforcement learning  
   D. Dimensionality reduction  

<details>
<summary>Answer</summary>
B. Classification and regression
</details>

---

### Question 8:
*How does the K-NN algorithm make predictions?*

   A. By learning an explicit mapping from the training data  
   B. By storing all training data and finding the K nearest neighbors at test time  
   C. By clustering the data into K groups  
   D. By reducing the dimensionality of the data  

<details>
<summary>Answer</summary>
B. By storing all training data and finding the K nearest neighbors at test time
</details>

---

### Question 9:
*Which of the following is a parameter of the K-NN algorithm?*

   A. The number of clusters (K)  
   B. The number of neighbors (K)  
   C. The learning rate  
   D. The regularization parameter  

<details>
<summary>Answer</summary>
B. The number of neighbors (K)
</details>

---

### Question 10:
*What is the most commonly used distance metric in K-NN for continuous numerical data?*

   A. Manhattan distance  
   B. Euclidean distance  
   C. Hamming distance  
   D. Cosine similarity  

<details>
<summary>Answer</summary>
B. Euclidean distance
</details>

---

### Question 11:
*Which of the following distance metrics is suitable for categorical or binary data?*

   A. Euclidean distance  
   B. Manhattan distance  
   C. Hamming distance  
   D. Cosine similarity  

<details>
<summary>Answer</summary>
C. Hamming distance
</details>

---

### Question 12:
*What is the main disadvantage of the K-NN algorithm?*

   A. It requires a large amount of memory to store all training data  
   B. It is not suitable for classification tasks  
   C. It cannot handle high-dimensional data  
   D. It is only applicable to regression tasks  

<details>
<summary>Answer</summary>
A. It requires a large amount of memory to store all training data
</details>

---

### Question 13:
*What happens if the value of K in K-NN is too small?*

   A. The model may overfit the data  
   B. The model may underfit the data  
   C. The model will perform well on unseen data  
   D. The model will have high bias  

<details>
<summary>Answer</summary>
A. The model may overfit the data
</details>

---

### Question 14:
*What happens if the value of K in K-NN is too large?*

   A. The model may overfit the data  
   B. The model may underfit the data  
   C. The model will perform well on unseen data  
   D. The model will have low bias  

<details>
<summary>Answer</summary>
B. The model may underfit the data
</details>

---

### Question 15:
*Which of the following is a way to choose the optimal value of K in K-NN?*

   A. Use cross-validation  
   B. Use a fixed value of K for all datasets  
   C. Use the smallest possible value of K  
   D. Use the largest possible value of K  

<details>
<summary>Answer</summary>
A. Use cross-validation
</details>

---

### Question 16:
*What is the main advantage of the K-NN algorithm?*

   A. It is simple and intuitive  
   B. It requires minimal memory  
   C. It is not sensitive to noisy features  
   D. It performs well in high-dimensional data  

<details>
<summary>Answer</summary>
A. It is simple and intuitive
</details>

---

### Question 17:
*Which of the following is a disadvantage of the K-NN algorithm?*

   A. It is computationally expensive at test time  
   B. It cannot handle categorical data  
   C. It is not suitable for regression tasks  
   D. It requires labeled data for training  

<details>
<summary>Answer</summary>
A. It is computationally expensive at test time
</details>

---

### Question 18:
*What is the effect of feature normalization on the K-NN algorithm?*

   A. It reduces the computational cost  
   B. It ensures that all features are on the same scale  
   C. It increases the model's complexity  
   D. It reduces the number of neighbors (K)  

<details>
<summary>Answer</summary>
B. It ensures that all features are on the same scale
</details>

---

### Question 19:
*Which of the following is true about the K-NN algorithm?*

   A. It learns an explicit mapping from the training data  
   B. It is a parametric method  
   C. It is a non-parametric method  
   D. It is only used for classification tasks  

<details>
<summary>Answer</summary>
C. It is a non-parametric method
</details>

---

### Question 20:
*What is the main reason for using cross-validation in K-NN?*

   A. To reduce the computational cost  
   B. To choose the optimal value of K  
   C. To normalize the features  
   D. To reduce the memory usage  

<details>
<summary>Answer</summary>
B. To choose the optimal value of K
</details>

---